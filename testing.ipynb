{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved model\n",
    "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)  # Adjust to match the original model's output units\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\Admin\\PythonScripts\\RohitPSACM\\pepe_non_pepe_classifier.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\Admin\\PythonScripts\\RohitPSACM\\test\\not_pepe\\isNotPepe96.png\"  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transforms\n",
    "#define data augmentations\n",
    "\n",
    "dataTransforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "      transforms.Resize(224),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "\n",
    "    \"val\": transforms.Compose([\n",
    "      transforms.Resize(224),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "\n",
    "\n",
    "    \"test\" : transforms.Compose([\n",
    "      transforms.Resize(224),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 805, 'val': 383, 'test': 180}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pepe', 'not_pepe']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = r\"C:\\Users\\Admin\\PythonScripts\\RohitPSACM\"\n",
    "\n",
    "imgDatasets = {x: datasets.ImageFolder(os.path.join(folder_path, x), dataTransforms[x]) for x in ['train', 'val', 'test'] }\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(imgDatasets[x], batch_size=4, shuffle=True, num_workers=2) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(imgDatasets[x]) for x in ['train', 'val', 'test']}\n",
    "print(dataset_sizes)\n",
    "\n",
    "class_names = imgDatasets['test'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Use all parameters\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)  # Adjust to match the original model's output units\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\Admin\\PythonScripts\\RohitPSACM\\flower_classification_model0.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Create a new model with the correct final layer\n",
    "new_model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "new_model.fc = nn.Linear(new_model.fc.in_features, 2)  # Adjust to match the desired output units\n",
    "\n",
    "# Copy the weights and biases from the loaded model to the new model\n",
    "new_model.fc.weight.data = model.fc.weight.data[0:2]  # Copy only the first 2 output units\n",
    "new_model.fc.bias.data = model.fc.bias.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0280 Acc: 0.9925\n",
      "val Loss: 0.0253 Acc: 0.9948\n",
      "test Loss: 0.0549 Acc: 0.9833\n",
      "train Loss: 0.0313 Acc: 0.9888\n",
      "val Loss: 0.0213 Acc: 0.9974\n",
      "test Loss: 0.0627 Acc: 0.9722\n",
      "train Loss: 0.0287 Acc: 0.9888\n",
      "val Loss: 0.0307 Acc: 0.9948\n",
      "test Loss: 0.0620 Acc: 0.9778\n",
      "train Loss: 0.0243 Acc: 0.9938\n",
      "val Loss: 0.0319 Acc: 0.9948\n",
      "test Loss: 0.0667 Acc: 0.9778\n",
      "train Loss: 0.0289 Acc: 0.9901\n",
      "val Loss: 0.0270 Acc: 0.9948\n",
      "test Loss: 0.0458 Acc: 0.9833\n",
      "train Loss: 0.0241 Acc: 0.9913\n",
      "val Loss: 0.0305 Acc: 0.9948\n",
      "test Loss: 0.0578 Acc: 0.9833\n",
      "train Loss: 0.0312 Acc: 0.9913\n",
      "val Loss: 0.0295 Acc: 0.9948\n",
      "test Loss: 0.0643 Acc: 0.9722\n",
      "train Loss: 0.0251 Acc: 0.9925\n",
      "val Loss: 0.0322 Acc: 0.9948\n",
      "test Loss: 0.0671 Acc: 0.9778\n",
      "train Loss: 0.0307 Acc: 0.9913\n",
      "val Loss: 0.0279 Acc: 0.9948\n",
      "test Loss: 0.0669 Acc: 0.9722\n",
      "train Loss: 0.0242 Acc: 0.9913\n",
      "val Loss: 0.0213 Acc: 0.9974\n",
      "test Loss: 0.0694 Acc: 0.9722\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        if phase == 'train':\n",
    "            new_model.train()\n",
    "        else:\n",
    "            new_model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "----------\n",
      "train Loss: 0.4945 Acc: 0.7516\n",
      "val Loss: 0.3272 Acc: 0.8825\n",
      "test Loss: 0.3666 Acc: 0.8333\n",
      "Epoch 2/15\n",
      "----------\n",
      "train Loss: 0.4665 Acc: 0.7752\n",
      "val Loss: 0.2929 Acc: 0.9008\n",
      "test Loss: 0.3456 Acc: 0.8722\n",
      "Epoch 3/15\n",
      "----------\n",
      "train Loss: 0.4279 Acc: 0.8050\n",
      "val Loss: 0.3256 Acc: 0.8642\n",
      "test Loss: 0.4148 Acc: 0.8611\n",
      "Epoch 4/15\n",
      "----------\n",
      "train Loss: 0.4787 Acc: 0.7615\n",
      "val Loss: 0.3704 Acc: 0.8825\n",
      "test Loss: 0.4493 Acc: 0.8500\n",
      "Epoch 5/15\n",
      "----------\n",
      "train Loss: 0.4157 Acc: 0.8124\n",
      "val Loss: 0.3135 Acc: 0.9008\n",
      "test Loss: 0.3902 Acc: 0.8389\n",
      "Epoch 6/15\n",
      "----------\n",
      "train Loss: 0.4244 Acc: 0.8112\n",
      "val Loss: 0.3150 Acc: 0.8877\n",
      "test Loss: 0.4028 Acc: 0.8333\n",
      "Epoch 7/15\n",
      "----------\n",
      "train Loss: 0.4009 Acc: 0.8112\n",
      "val Loss: 0.2875 Acc: 0.8956\n",
      "test Loss: 0.3786 Acc: 0.8333\n",
      "Epoch 8/15\n",
      "----------\n",
      "train Loss: 0.3819 Acc: 0.8311\n",
      "val Loss: 0.2839 Acc: 0.8982\n",
      "test Loss: 0.3690 Acc: 0.8667\n",
      "Epoch 9/15\n",
      "----------\n",
      "train Loss: 0.3850 Acc: 0.8335\n",
      "val Loss: 0.2930 Acc: 0.8903\n",
      "test Loss: 0.3841 Acc: 0.8333\n",
      "Epoch 10/15\n",
      "----------\n",
      "train Loss: 0.3708 Acc: 0.8248\n",
      "val Loss: 0.2767 Acc: 0.8903\n",
      "test Loss: 0.3702 Acc: 0.8389\n",
      "Epoch 11/15\n",
      "----------\n",
      "train Loss: 0.3832 Acc: 0.8261\n",
      "val Loss: 0.2603 Acc: 0.9060\n",
      "test Loss: 0.3329 Acc: 0.8611\n",
      "Epoch 12/15\n",
      "----------\n",
      "train Loss: 0.3636 Acc: 0.8472\n",
      "val Loss: 0.2324 Acc: 0.9138\n",
      "test Loss: 0.3413 Acc: 0.8389\n",
      "Epoch 13/15\n",
      "----------\n",
      "train Loss: 0.3432 Acc: 0.8422\n",
      "val Loss: 0.2435 Acc: 0.9086\n",
      "test Loss: 0.3249 Acc: 0.8500\n",
      "Epoch 14/15\n",
      "----------\n",
      "train Loss: 0.3537 Acc: 0.8360\n",
      "val Loss: 0.3089 Acc: 0.9034\n",
      "test Loss: 0.3511 Acc: 0.8556\n",
      "Epoch 15/15\n",
      "----------\n",
      "train Loss: 0.3406 Acc: 0.8571\n",
      "val Loss: 0.2356 Acc: 0.9060\n",
      "test Loss: 0.3098 Acc: 0.8778\n",
      "Train Losses:  {1: '0.4945', 2: '0.4665', 3: '0.4279', 4: '0.4787', 5: '0.4157', 6: '0.4244', 7: '0.4009', 8: '0.3819', 9: '0.3850', 10: '0.3708', 11: '0.3832', 12: '0.3636', 13: '0.3432', 14: '0.3537', 15: '0.3406'}\n",
      "Train Accuracy:  {1: '0.7516', 2: '0.7752', 3: '0.8050', 4: '0.7615', 5: '0.8124', 6: '0.8112', 7: '0.8112', 8: '0.8311', 9: '0.8335', 10: '0.8248', 11: '0.8261', 12: '0.8472', 13: '0.8422', 14: '0.8360', 15: '0.8571'}\n",
      "Validation Losses:  {1: '0.3272', 2: '0.2929', 3: '0.3256', 4: '0.3704', 5: '0.3135', 6: '0.3150', 7: '0.2875', 8: '0.2839', 9: '0.2930', 10: '0.2767', 11: '0.2603', 12: '0.2324', 13: '0.2435', 14: '0.3089', 15: '0.2356'}\n",
      "Validation Accuracy:  {1: '0.8825', 2: '0.9008', 3: '0.8642', 4: '0.8825', 5: '0.9008', 6: '0.8877', 7: '0.8956', 8: '0.8982', 9: '0.8903', 10: '0.8903', 11: '0.9060', 12: '0.9138', 13: '0.9086', 14: '0.9034', 15: '0.9060'}\n",
      "Testing Losses:  {1: '0.3666', 2: '0.3456', 3: '0.4148', 4: '0.4493', 5: '0.3902', 6: '0.4028', 7: '0.3786', 8: '0.3690', 9: '0.3841', 10: '0.3702', 11: '0.3329', 12: '0.3413', 13: '0.3249', 14: '0.3511', 15: '0.3098'}\n",
      "Testing Accuracy:  {1: '0.8333', 2: '0.8722', 3: '0.8611', 4: '0.8500', 5: '0.8389', 6: '0.8333', 7: '0.8333', 8: '0.8667', 9: '0.8333', 10: '0.8389', 11: '0.8611', 12: '0.8389', 13: '0.8500', 14: '0.8556', 15: '0.8778'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "trainLoss = dict([])\n",
    "trainAcc = dict([])\n",
    "valLoss = dict([])\n",
    "valAcc = dict([])\n",
    "testLoss = dict([])\n",
    "testAcc = dict([])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Set data directories\n",
    "data_dir = folder_path\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "# Load datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Define model architecture (using pre-trained ResNet18)\n",
    "model = models.resnet18(weights = None)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: Pepe and Non-Pepe\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "#num_epochs = 15\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, num_epochs = 15):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                trainLoss[epoch + 1] = f\"{epoch_loss:.4f}\"\n",
    "                trainAcc[epoch + 1] = f\"{epoch_acc:.4f}\"\n",
    "\n",
    "            elif phase == 'val':\n",
    "                valLoss[epoch + 1] = f\"{epoch_loss:.4f}\"\n",
    "                valAcc[epoch + 1] = f\"{epoch_acc:.4f}\"\n",
    "\n",
    "            elif phase == 'test':\n",
    "                testLoss[epoch + 1] = f\"{epoch_loss:.4f}\"\n",
    "                testAcc[epoch + 1] = f\"{epoch_acc:.4f}\"\n",
    "\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, num_epochs=15)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'pepe_non_pepe_classifier.pth')\n",
    "\n",
    "print(\"Train Losses: \", trainLoss)\n",
    "print(\"Train Accuracy: \", trainAcc)\n",
    "print(\"Validation Losses: \", valLoss)\n",
    "print(\"Validation Accuracy: \", valAcc)\n",
    "print(\"Testing Losses: \", testLoss)\n",
    "print(\"Testing Accuracy: \", testAcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.4945, 2: 0.4665, 3: 0.4279, 4: 0.4787, 5: 0.4157, 6: 0.4244, 7: 0.4009, 8: 0.3819, 9: 0.385, 10: 0.3708, 11: 0.3832, 12: 0.3636, 13: 0.3432, 14: 0.3537, 15: 0.3406}\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'pepe_non_pepe_classifier.pth')\n",
    "print(trainLoss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
